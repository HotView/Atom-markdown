作用就三种：分类，回归，聚类分析
## 监督学习
- 支持向量机
- 人工神经网络
- 决策树
- 逻辑回归
- 朴素贝叶斯
- k近邻
- 随机森林：由很多的决策的树构成
- 线性回归
- 回归树
## 非监督学习
==非监督式学习的目标是对数据中潜在的结构和分布建模，以便对数据作更进一步的学习。==
- K-means：基于距离的聚类
- DBSCAN：基于密度的聚类
- BIRCH：层次聚类
- PCA
提取有意义的特征进行进一步的学习。
- 因子分析：
当特征数远远多于样本数时，我们将特征数目降低，来处理计算。与主成分分析很像，但是有些不同。
  - 因子分析中是把变量表示成各因子的线性组合，而主成分分析中则是把主成分表示成各变量的线性组合。
  - 主成分分析的重点在于解释各变量的总方差，而因子分析则把重点放在解释各变量之间的协方差。
## 支持向量机
通过寻找分类超平面进而最大化类别间隔实现分类
##人工神经网络
## 逻辑回归
通过拟合曲线实现分类（或者学习超平面）
## 决策树
通过寻找最佳划分特征进而学习样本路径实现分类，主要的决策树模型有三种：
- ID3
- ID4.5
- CART算法：既能创建分类树，也可以创建回归树
## 朴树贝叶斯
朴素：特征相互独立
通过考虑特征概率来预测分类，比如判断坏人通过一个笑的特征概率来判断好人和坏人，然后朴素贝叶斯就把笑的特征概率化，构成一个“人的样貌向量”以及对应的好人坏人标签，训练处一个好人坏人模型。
- 找到一个一直分类的待分类集合，这个集合叫作训练集合
- 统计得到在各类别下各个特征属性的条件概率估计$P(a_1|y_1),P(a_2|y_1),…,P(a_m|y_1) $,目的是计算$P(y_i|x)$
- 如果各个特征属性之间相互独立$$P(y_i|x) = \frac{P(x|y_i)P(y_i)}{P(x)} = P(a_1|y_i)P(a_2|y_i)....P(a_i|y_i)P(y_i))$$
- 对于一个待分类的X= {a1,a2...am}
- 计算$P(y_1|x),P(y_2|x)..P(y_n|x)$
- 取上面的计算结果最大即为应该计算的分类
## k近邻
## 随机森林
对于一个复杂的分类问题，训练一个复杂的分类模型通常比较好费时间，同时，为了提高准确性，通常可以选择多个分类模型，并将各自的预测结果组合起来，得到最终的预测结果。

集成学习就是这样一种学习方法：将多种学习算法，通过适当的方式组合起来，主要分为Bagging和Boosting

随机森林就是Bagging中最重要的一种算法。由很多的分类树构成。

## 线性回归
## 回归树
