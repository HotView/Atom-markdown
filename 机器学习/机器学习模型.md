## 监督学习
#### 支持向量机
通过寻找分类超平面进而最大化类别间隔实现分类
#### 人工神经网络
#### 逻辑回归
通过拟合曲线实现分类（或者学习超平面）
#### 决策树
通过寻找最佳划分特征进而学习样本路径实现分类，主要的决策树模型有三种：
- ID3
- ID4.5
- CART算法：既能创建分类树，也可以创建回归树
#### 朴树贝叶斯
朴素：特征相互独立
先验概率：P(A)
后验概率：P（A|B）
似然度：P(B|A)
标准化常量：P(B)  
通过考虑特征概率来预测分类，比如判断坏人通过一个笑的特征概率来判断好人和坏人，然后朴素贝叶斯就把笑的特征概率化，构成一个“人的样貌向量”以及对应的好人坏人标签，训练处一个好人坏人模型。
- 找到一个一直分类的待分类集合，这个集合叫作训练集合
- 统计得到在各类别下各个特征属性的条件概率估计$P(a_1|y_1),P(a_2|y_1),…,P(a_m|y_1) $,目的是计算$P(y_i|x)$
- 如果各个特征属性之间相互独立$$P(y_i|x) = \frac{P(x|y_i)P(y_i)}{P(x)} = P(a_1|y_i)P(a_2|y_i)....P(a_i|y_i)P(y_i))$$
- 对于一个待分类的X= {a1,a2...am}
- 计算$P(y_1|x),P(y_2|x)..P(y_n|x)$
- 取上面的计算结果最大即为应该计算的分类
#### k近邻
#### 随机森林
对于一个复杂的分类问题，训练一个复杂的分类模型通常比较好费时间，同时，为了提高准确性，通常可以选择多个分类模型，并将各自的预测结果组合起来，得到最终的预测结果。

集成学习就是这样一种学习方法：将多种学习算法，通过适当的方式组合起来，主要分为Bagging和Boosting

随机森林就是Bagging中最重要的一种算法。由很多的分类树构成。

#### 线性回归
#### 回归树
## 非监督学习
#### 聚类
#### PCA
#### 隐马尔科夫模型
推测的变量必须是离散的状态转移，观测值的可以是连续型的变量。x,z-->f z是一个隐含变量（因为z看不见，所以就被称为无监督学习），可以通过x推测出数据变量，例如：天气（z）本身不能直接观测，而是看豆子的干燥程度（x）去推测。
主要有三个问题：

问题一：知道骰子有几种（隐含状态数量），每种骰子是什么（转换概率），根据骰子掷出的结果（可见状态链），想知道每次掷出的都是哪种骰子（隐含状态链）

问题二：知道骰子有几种（隐含状态数量），每种骰子是什么（转换概率），根据骰子掷出的结果（可见状态链），想知道掷出这个结果的原因

问题三：知道骰子有几种（隐含状态数量），不知道每种骰子是什么（转换概率），根据骰子掷出的结果（可见状态链），想反推出每种骰子是什么（转换概率）

要解决的问题：
- 通过观测的序列来推测出最可能的结果的变化，比如写一个数字，随着写的笔画越多，推测出某一个数字的可能性越来越大。
- 学习出一个转移矩阵和观测矩阵

例子：
- 例子：GPS与汽车位置关联关系。

计算模型
- 简单的概率图模型
- 前后向算法：本质就是简单的动态规划，将指数级复杂度降到多项式级的复杂度。
#### 因子分析
