在反向传播的优化损失函数算法中，有三种算法，分别为：梯度下降，随机梯度下降，批梯度下降算法。
- 梯度下降法：不能保证达到全局最优解，只有当损失函数为凸函数时，梯度下降算法才能保证达到全局最优解；时间太长。
- 在某一条数据上损失函数更新变小并不代表在全部数据上的损失函数更小，无法达到全局最优解。
## Batch的概念
什么是Batch，有两种解释，比较形象具体。
1. 对于一个有2000个训练样本的数据集。将2000个样本分成大小为500的batch，那么完成一个epoch需要4个iteration。
2. 如果把准备训练数据比喻成一块准备打火锅的牛肉，那么epoch就是整块牛肉，batch就是切片后的牛肉片，iteration就是涮一块牛肉片
![](picture/什么是Batch-dab883c9.png)
## Batch用来做什么？
不是给人吃，是喂给模型吃。在搭建了“模型-策略-算法”三大步之后，要利用数据跑这个框架，训练处最佳参数。
1. 理想状态下，就是把所有数据都喂给框架，求出最小损失，再更新参数，重复这个过程，但是就像煮一整块牛肉那样，不知道什么时候能够煮熟：==全数据量下降==
2. 另一个极端，就是每次只给模型喂一条数据，立马就熟了，快是够快了，但是一不小心直接化掉，没得吃（可能无法得到局部最优解）：==随机梯度下降算法==
3. 平衡上述两个方案，将数据切成batch大小的一块，每次（iteration）只吃一块。每次只计算一小部分的损失函数，并更改参数。
## Batch的实现
- yield--》generator
- slice_input_producer+batch
## 两种方式的对比
方式1： yield→generator  30个epoch
试验效果，开始前python.exe进程占了402M内存。
试验中，内存基本维持在865M左右
试验后，30个epoch耗时需要49.8s
方式2： slice_input_producer + batch
进行slice_input_producer这步，占用内存由410M提升到了583M
训练的时候，内存占用比较飘忽，有时1G多。
20个epoch耗时需要199s
==小结：方式1的效率暂时比方式2快不少。==
