CodeSymbol|Math Symbol|	Definition	|Dimensions
--|--|--|--
X|	$$X$$| Input Data, each row in an example|	(numExamples, inputLayerSize)|
y|	$$y$$|	target data|	(numExamples, outputLayerSize)|
W1|	$$W^{(1)}$$	|Layer 1 weights	|(inputLayerSize, hiddenLayerSize)
W2|	$$W^{(2)}$$	|Layer 2 weights|	(hiddenLayerSize, outputLayerSize)
z2|	$$z^{(2)}$$|	Layer 2 activation	|(numExamples, hiddenLayerSize)
a2|	$$a^{(2)}$$	|Layer 2 activity|	(numExamples, hiddenLayerSize)
z3|	$$z^{(3)}$$|	Layer 3 activation|	(numExamples, outputLayerSize)
J|	$$J$$|	Cost	|(1, outputLayerSize)  
dJdz3|	$$\frac{\partial J}{\partial z^{(3)} } = \delta^{(3)}$$	|Partial derivative of cost with respect to $z^{(3)}$	|(numExamples,outputLayerSize)
dJdW2|	$$\frac{\partial J}{\partial W^{(2)}}$$	|Partial derivative of cost with respect to $W^{(2)}$	|(hiddenLayerSize, outputLayerSize)
dz3dz2|	$$\frac{\partial z^{(3)}}{\partial z^{(2)}}$$|	Partial derivative of $z^{(3)}$ with respect to $z^{(2)}$|	(numExamples, hiddenLayerSize)
dJdW1|	$$\frac{\partial J}{\partial W^{(1)}}$$|	Partial derivative of cost with respect to $W^{(1)}$|	(inputLayerSize, hiddenLayerSize)
delta2|	$$\delta^{(2)}$$|	Backpropagating Error 2	|(numExamples,hiddenLayerSize)
delta3|	$$\delta^{(3)}$$|	Backpropagating Error 1	|(numExamples,outputLayerSize)
## 各层之间的数据流动
- 表示下一层的结点值由上一层的输出与权重的乘积得到
$$
z^{(2)} = XW^{(1)} \tag{1}\\
$$
- 表示本层的神经网络激活值，由本层的结点值对应到激活函数的输出得到，相当于一个映射
$$
a^{(2)} = f(z^{(2)})
$$
而最后一层的激活值就是整个网络的输出值,用yhat来代替a（activity:激活值）
$$
\hat y = f(z^{(3)})
$$
损失函数的表示，用平方表示的话，就是将一个函数设置为凸函数。
$$
J = \sum \frac{1}{2}(y-\hat{y})^2 \tag{5}
$$
## 反向传播
- 损失函数与第某一层W之间的偏导数，即损失函数对于某一层W的微小变动的敏感度
$$
\frac{\partial J}{\partial W^{(k)}} =
(a^{(k)})^T\delta^{(k+1)}\tag{6}
$$
- 本例中，我们以第二层的权重值为例：
$$
\frac{\partial J}{\partial W^{(2)}} =
(a^{(2)})^T\delta^{(3)}\tag{6}
$$
$$
\delta^{(3)} = -(y-\hat{y}) f^\prime(z^{(3)})
$$$$
\delta^{(2)} = \delta^{(3)}
(W^{(2)})^{T}
f^\prime(z^{(2)})
$$$$
\delta^{(2)}= -(y-\hat{y}) f^\prime(z^{(3)})(W^{(2)})^{T}
 f^\prime(z^{(2)})$$
- 通过链式法则，我们可以得到：
$$
\frac{\partial J}{\partial W^{(2)}} = -(y-\hat{y}) \frac{\partial \hat{y}}{\partial W^{(2)}}
$$
- 进而转化为后一层节点的值对当前层权重W的偏导：
$$
\frac{\partial J}{\partial W^{(2)}} =
-(y-\hat{y})
\frac{\partial \hat{y}}{\partial z^{(3)}}  
\frac{\partial z^{(3)}}{\partial W^{(2)}}
$$因为我们拿一个单独的突触来看，对于每一个单独的突触的值，都有$$
z^{(3)} = a^{(2)}W^{(2)} \tag{3}\\
$$
所以$dz/dW(2)$只是激活值，即$a^{(2)}$。
- 对于每一层都套用上述的流程，我们可以证明，对于损失函数对于每一层的权重的微小变化的敏感度，最后归结于最后一层的突触值对于每一层的权重微小变换的敏感度，所以每一册变化量只是最后一层的突触值对于权重的偏导数：
$$
\frac{\partial z^{(k+2)}}{\partial W^{(k)}} = \frac{\partial z^{(k+2)}}{\partial a^{(k+1)}}\frac{\partial a^{(k+1)}}{\partial W^{(k)}}
$$
终结处都是激活值对于权重的偏导数，然后转换为突触值对于权重的偏导数。
==所以规则就是由z链出前一层的a，再由a链出本层的z，不断地迭代链下去。==
