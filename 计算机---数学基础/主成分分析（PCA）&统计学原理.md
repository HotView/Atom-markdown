## PCA
- PCA的数学定义是：一个正交化线性变换，把数据变换到一个新的坐标系统中，使得这一数据的任何投影的第一大方差在第一个坐标（称为第一主成分）上，第二大方差在第二个坐标（第二主成分）上，依次类推[4]。

- PCA的全部工作简单点说，就是对原始的空间中顺序地找一组相互正交的坐标轴，第一个轴是使得方差（==数据在这个轴上的方差==）最大的，第二个轴是在与第一个轴正交的平面中使得方差最大的，第三个轴是在与第1、2个轴正交的平面中方差最大的，这样假设在N维空间中，我们可以找到N个这样的坐标轴，我们取前r个去近似这个空间，这样就从一个N维的空间压缩到r维的空间了，但是我们选择的r个坐标轴能够使得空间的压缩使得数据的损失最小。

两种计算方法
- 利用奇异值分解的方法，效果比较好
- 利用统计方法计算PCA

化繁为简，降低维数，抽象出核心资讯。
观察两个变数的相关性，可以画散点图。当维数较大时，画图不适用。
这就是主成分分析的目的。
找出降维后的加权系数。
## 目的
X经过变换得到Y

我们使得Y的协方差矩阵对角线上的值尽可能大（==优化目标是使得对角线上元素之和最大==），非对角线上的值尽可能为零，这样就达到了减少数据维数并保留绝大部分信息的目的。D是一个对角矩阵就好了，即每一维的数据都线形无关，且通过拉格朗日证明，当方差取的最大时的约束是特征值特征向量的关系。

C= XX'
D = YY' = PX'(PX)' = PXX'P = PCP'
我们寻找的矩阵P就是使得C对角化的矩阵
C = P'DP

## PCA流程（利用统计方法）
- 方差
![enter image description here](https://www.zhihu.com/equation?tex=%5Csigma_x%5E2=%5Cfrac%7B1%7D%7Bn-1%7D%5Csum_%7Bi=1%7D%5En%5Cleft%28x_i-%5Cbar%7Bx%7D%5Cright%29%5E2)
- 协方差矩阵原理
![enter image description here](https://www.zhihu.com/equation?tex=%5Csigma%5Cleft%28x_m,x_k%5Cright%29=%5Cfrac%7B1%7D%7Bn-1%7D%5Csum_%7Bi=1%7D%5En%5Cleft%28x_%7Bmi%7D-%5Cbar%7Bx%7D_m%5Cright%29%5Cleft%28x_%7Bki%7D-%5Cbar%7Bx%7D_k%5Cright%29)


几个因素的大量数据，构建因素的协方差矩阵。寻找一个新的特征空间：
（协方差矩阵还可以这样计算，先让样本矩阵中心化，即每一维度减去该维度的均值，使每一维度上的均值为0，然后直接用新的到的样本矩阵乘上它的转置，然后除以(N-1)即可）
- 减去平均值。
- 标准化，相对的好坏。
- 归一化，长度变为1。
- 求取相关矩阵，对称矩阵。==怎么加权，知道哪一个影响度最好？==。
- 分解相关矩阵，形成新的因素序列。即新的特征空间。
- 求出新特征空间的变换矩阵。

将原来的数据通过变换到新的特征空间来看待特征空间里数据的分布。

**主成分贡献度：**$\frac{\lambda 1\lambda 2\lambda ..\lambda k}{\lambda 1\lambda 2\lambda ..\lambda n}$
>不是方阵就只能SVD分解了，对原始的因素的大量因素（不进行求平均你绝，减平均，求相关矩阵）进行SVD分解。

多元正态分布的概率密度是由**协方差矩阵的特征向量控制旋转(rotation)**，**特征值控制尺度(scale)**，除了协方差矩阵，**均值向量会控制概率密度的位置**
## 讨论
####  为什么减平均
计算协方差的步骤。
#### 为什么计算协方差矩阵（PCA）


维数减少了，虽然可以大大减少算法的计算量，但是若对基矩阵P选择不当的话就很有可能会导致信息量的缺失。

因此我们要选择哪K个基(这里还不知道是特征向量)才能保证降维后能最大程度保留原有的信息，是进行设计的主方向。

什么是信息呢？根据信息论的定义，我们可以知道信息来源于未知。也就是说如果不同样本的同一维度的值差异特别大，那该维度带给我们的信息量就是极大的。转换成数学语言，也就是说某维度的方差越大，它的信息量越大。

==第一个优化目标：降维后各维度的方差尽可能大。==我们的第二个优化目标便是保证不同维度之间的相关性为0(其实就是让基向量互相正交)。
==PCA算法的优化目标就是:==
-  ① 降维后同一纬度的方差最大
- ② 不同维度之间的相关性为0

满足同一纬度的方差最大的条件是矩阵的迹最大，根据拉格朗日求导得，一阶导数等于零，得到满足的条件是特征值特征向量的式子。

使得降维后的方差最大，协方差矩阵的对角元素就是同一维的方差。
根据线性代数，我们可以知道同一元素的协方差就表示该元素的方差，不同元素之间的协方差就表示它们的相关性。因此这两个优化目标可以用协方差矩阵来表示

信息量保存能力最大的基向量一定是样本矩阵X的协方差矩阵的特征向量，并且这个特征向量保存的信息量就是它对应的特征值的绝对值。这个推导过程就解释了为什么PCA算法要利用样本协方差的特征向量矩阵来降维。

  

  

> Written with [StackEdit](https://stackedit.io/).
<!--stackedit_data:
eyJoaXN0b3J5IjpbNTU5MTcyMDc5XX0=
-->